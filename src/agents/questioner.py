"""
Agente preguntador interactivo para recopilar informaci√≥n del usuario
"""
from typing import Dict, Any, List, Optional
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field

from src.agents.base_agent import BaseAgent


class ConversationContext(BaseModel):
    """Contexto de la conversaci√≥n"""
    questions_asked: List[str] = Field(default_factory=list, description="Preguntas ya realizadas")
    user_answers: List[str] = Field(default_factory=list, description="Respuestas del usuario")
    topics_covered: List[str] = Field(default_factory=list, description="Temas ya cubiertos")
    current_question_number: int = Field(default=0, description="N√∫mero de pregunta actual")


class QuestionerAgent(BaseAgent):
    """
    Agente inteligente que hace preguntas din√°micas al usuario
    para recopilar informaci√≥n sobre sus necesidades.
    
    Caracter√≠sticas:
    - M√°ximo 5 preguntas
    - Preguntas adaptativas basadas en respuestas previas
    - Conversaci√≥n natural y contextual
    - Extracci√≥n inteligente de informaci√≥n
    """
    
    MAX_QUESTIONS = 5
    
    def __init__(self):
        super().__init__(
            name="Agente Preguntador Interactivo",
            role="Recopilar informaci√≥n mediante preguntas inteligentes y adaptativas"
        )
        
        self.conversation_context = ConversationContext()
        
        # Prompt mejorado para generar preguntas ultra-personalizadas con Gemini
        self.question_prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un asistente de compras experto y emp√°tico que hace preguntas INTELIGENTES 
            y PERSONALIZADAS para entender las necesidades del usuario. Tu objetivo es descubrir qu√© 
            producto necesita realmente y por qu√©.
            
            üéØ ESTRATEGIA DE PREGUNTAS:
            
            1. **ANALIZA EL CONTEXTO**: Lee cuidadosamente las respuestas previas
            2. **PROFUNDIZA**: Si el usuario mencion√≥ algo interesante, pregunta m√°s detalles
            3. **CONECTA IDEAS**: Relaciona la nueva pregunta con lo que ya sabes
            4. **S√â ESPEC√çFICO**: Usa la informaci√≥n que ya tienes para hacer preguntas m√°s precisas
            5. **PRIORIZA**: Enf√≥cate en lo que a√∫n falta y es cr√≠tico
            
            üìä INFORMACI√ìN CR√çTICA A OBTENER:
            - **Categor√≠a**: ¬øQu√© tipo de producto? (laptop, tel√©fono, etc.)
            - **Presupuesto**: ¬øRango de precio aproximado?
            - **Uso principal**: ¬øPara qu√© lo usar√°? (trabajo, gaming, estudio, etc.)
            - **Caracter√≠sticas clave**: ¬øQu√© especificaciones son importantes?
            - **Preferencias**: ¬øMarcas, tama√±os, colores, etc.?
            - **Restricciones**: ¬øLimitaciones de tiempo, espacio, compatibilidad?
            
            üí° EJEMPLOS DE PREGUNTAS CONTEXTUALES:
            
            Ejemplo 1:
            Usuario dijo: "Necesito una laptop"
            Mal: "¬øQu√© tipo de producto buscas?"
            Bien: "Perfecto, ¬øpara qu√© usar√°s principalmente tu laptop? ¬øTrabajo, estudio, gaming o entretenimiento?"
            
            Ejemplo 2:
            Usuario dijo: "Para programar"
            Mal: "¬øQu√© caracter√≠sticas quieres?"
            Bien: "Excelente, para programaci√≥n. ¬øQu√© tipo de desarrollo haces? ¬øTrabajas con IDEs pesados, 
            m√°quinas virtuales o desarrollo web principalmente?"
            
            Ejemplo 3:
            Usuario dijo: "Desarrollo web y algo de edici√≥n de video"
            Mal: "¬øCu√°nto quieres gastar?"
            Bien: "Interesante combinaci√≥n. Para edici√≥n de video necesitar√°s buena potencia. 
            ¬øCu√°l es tu presupuesto aproximado para una m√°quina que maneje ambas tareas?"
            
            ‚ö†Ô∏è EVITA:
            - Preguntas gen√©ricas que ignoran el contexto
            - Repetir informaci√≥n que ya diste
            - Preguntar lo que ya respondieron impl√≠citamente
            - Ser rob√≥tico o formal en exceso
            
            ‚úÖ REGLAS DE ORO:
            1. **USA lo que ya sabes**: Menciona detalles previos en tu pregunta
            2. **Una idea por pregunta**: No hagas preguntas compuestas
            3. **Conversacional**: Como si hablaras con un amigo
            4. **Emp√°tico**: Muestra que entiendes sus necesidades
            5. **Solo la pregunta**: No expliques, no des contexto extra
            
            üìù CONTEXTO ACTUAL:
            Pregunta n√∫mero: {questions_count}/{max_questions}
            Temas ya cubiertos: {topics_covered}
            
            CONVERSACI√ìN HASTA AHORA:
            {conversation_history}
            
            üéØ INSTRUCCI√ìN: Bas√°ndote en TODO el contexto anterior, genera UNA pregunta inteligente, 
            espec√≠fica y personalizada que profundice en la informaci√≥n m√°s valiosa que a√∫n falte."""),
            ("user", "Genera la siguiente pregunta personalizada:")
        ])
        
        # Prompt mejorado para analizar si necesitamos m√°s informaci√≥n
        self.analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un analista experto en comprensi√≥n de necesidades de clientes.
            
            üéØ TU TAREA: Determinar si tenemos SUFICIENTE informaci√≥n para recomendar productos.
            
            üìã INFORMACI√ìN M√çNIMA NECESARIA para una buena recomendaci√≥n:
            1. **Categor√≠a de producto** (qu√© tipo de producto busca)
            2. **Presupuesto** (rango de precio, aunque sea aproximado)
            3. **Uso principal** O **Caracter√≠sticas clave** (al menos uno de estos)
            
            ‚úÖ TENEMOS SUFICIENTE SI:
            - Sabemos QU√â busca, CU√ÅNTO puede gastar, y PARA QU√â lo necesita
            - O tenemos suficiente contexto para hacer recomendaciones relevantes
            - O el usuario fue muy espec√≠fico y claro en sus respuestas
            
            ‚ö†Ô∏è NECESITAMOS M√ÅS SI:
            - Falta informaci√≥n cr√≠tica (categor√≠a, presupuesto o uso)
            - Las respuestas fueron muy vagas o generales
            - Hay contradicciones que necesitan clarificaci√≥n
            - El usuario mencion√≥ algo importante que no hemos profundizado
            
            üìä CONTEXTO DE LA CONVERSACI√ìN:
            {conversation_history}
            
            Preguntas realizadas: {questions_count}/{max_questions}
            
            üéØ DECISI√ìN REQUERIDA:
            Responde SOLO con una de estas dos palabras seguida de una breve explicaci√≥n:
            - "CONTINUAR: [raz√≥n breve]" - Si necesitas informaci√≥n cr√≠tica adicional
            - "SUFICIENTE: [raz√≥n breve]" - Si ya puedes hacer buenas recomendaciones
            
            S√© cr√≠tico pero tambi√©n eficiente. No necesitamos informaci√≥n perfecta, solo suficiente."""),
            ("user", "¬øTenemos suficiente informaci√≥n o debemos continuar preguntando?")
        ])
        
        # Prompt para generar la primera pregunta (tambi√©n personalizada)
        self.initial_question_prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un asistente de compras amigable y profesional.
            
            üéØ TAREA: Genera una pregunta de APERTURA c√°lida y efectiva para iniciar la conversaci√≥n.
            
            ‚úÖ LA PREGUNTA DEBE:
            1. Ser amigable y acogedora
            2. Preguntar qu√© tipo de producto busca
            3. Ser abierta pero enfocada
            4. Incluir un saludo breve
            5. Mostrar entusiasmo por ayudar
            
            üí° EJEMPLOS DE BUENAS PREGUNTAS INICIALES:
            - "¬°Hola! üëã Estoy aqu√≠ para ayudarte a encontrar el producto perfecto. ¬øQu√© est√°s buscando hoy?"
            - "¬°Bienvenido! üòä Me encantar√≠a ayudarte. ¬øQu√© tipo de producto tienes en mente?"
            - "¬°Hola! Soy tu asistente de compras. ¬øEn qu√© producto est√°s interesado hoy?"
            
            ‚ö†Ô∏è EVITA:
            - Ser demasiado formal o rob√≥tico
            - Hacer m√∫ltiples preguntas a la vez
            - Ser muy largo o explicativo
            
            Genera SOLO la pregunta, sin texto adicional."""),
            ("user", "Genera la pregunta de apertura:")
        ])
    
    def generate_next_question(self) -> Optional[str]:
        """
        Genera la siguiente pregunta basada en el contexto de la conversaci√≥n usando Gemini
        
        Returns:
            Siguiente pregunta o None si no hay m√°s preguntas
        """
        if self.conversation_context.current_question_number >= self.MAX_QUESTIONS:
            return None
        
        # Si es la primera pregunta, usar prompt especial de apertura
        if self.conversation_context.current_question_number == 0:
            try:
                chain = self.initial_question_prompt | self.llm
                result = chain.invoke({})
                question = result.content.strip()
                
                self.conversation_context.current_question_number += 1
                self.conversation_context.questions_asked.append(question)
                return question
            except Exception as e:
                print(f"Error generando pregunta inicial: {e}")
                # Fallback a pregunta por defecto si falla
                question = "¬°Hola! üëã ¬øQu√© tipo de producto est√°s buscando hoy?"
                self.conversation_context.current_question_number += 1
                self.conversation_context.questions_asked.append(question)
                return question
        
        # Verificar si necesitamos m√°s informaci√≥n (despu√©s de 3 preguntas)
        if self.conversation_context.current_question_number >= 3:
            should_continue = self._should_continue_asking()
            if not should_continue:
                return None
        
        # Generar conversaci√≥n hist√≥rica con contexto rico
        conversation_history = self._format_conversation_history()
        
        # Generar siguiente pregunta personalizada con Gemini
        try:
            chain = self.question_prompt | self.llm
            result = chain.invoke({
                "questions_count": self.conversation_context.current_question_number,
                "max_questions": self.MAX_QUESTIONS,
                "conversation_history": conversation_history,
                "topics_covered": ", ".join(self.conversation_context.topics_covered) if self.conversation_context.topics_covered else "Ninguno a√∫n"
            })
            
            question = result.content.strip()
            
            # Limpiar la pregunta (remover comillas extras si las hay)
            question = question.strip('"').strip("'")
            
            self.conversation_context.current_question_number += 1
            self.conversation_context.questions_asked.append(question)
            
            return question
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Error generando pregunta con Gemini: {e}")
            return None
    
    def add_user_response(self, response: str):
        """
        A√±ade una respuesta del usuario al contexto y extrae informaci√≥n clave
        
        Args:
            response: Respuesta del usuario
        """
        self.conversation_context.user_answers.append(response)
        
        # Extraer temas mencionados (m√©todo simple)
        self._extract_topics(response)
        
        # An√°lisis m√°s profundo con Gemini (solo despu√©s de la segunda respuesta)
        if len(self.conversation_context.user_answers) >= 2:
            self._analyze_user_intent(response)
    
    def _should_continue_asking(self) -> bool:
        """
        Determina si debemos continuar haciendo preguntas
        
        Returns:
            True si debemos continuar, False si tenemos suficiente informaci√≥n
        """
        if self.conversation_context.current_question_number >= self.MAX_QUESTIONS:
            return False
        
        conversation_history = self._format_conversation_history()
        
        try:
            chain = self.analysis_prompt | self.llm
            result = chain.invoke({
                "conversation_history": conversation_history,
                "questions_count": self.conversation_context.current_question_number,
                "max_questions": self.MAX_QUESTIONS
            })
            
            analysis = result.content.strip()
            
            # Si el an√°lisis indica CONTINUAR, seguimos
            return "CONTINUAR" in analysis.upper()
            
        except Exception as e:
            print(f"Error analizando contexto: {e}")
            # En caso de error, continuamos si no hemos alcanzado el l√≠mite
            return self.conversation_context.current_question_number < self.MAX_QUESTIONS
    
    def _extract_topics(self, response: str):
        """
        Extrae temas mencionados en la respuesta para evitar preguntas redundantes
        
        Args:
            response: Respuesta del usuario
        """
        # Palabras clave para identificar temas
        topic_keywords = {
            "presupuesto": ["precio", "costo", "presupuesto", "dinero", "‚Ç¨", "$", "econ√≥mico", "barato", "caro"],
            "categor√≠a": ["laptop", "tel√©fono", "tablet", "auriculares", "teclado", "monitor", "televisor"],
            "uso": ["trabajo", "gaming", "estudio", "casa", "oficina", "port√°til", "uso", "utilizar"],
            "caracter√≠sticas": ["pantalla", "memoria", "almacenamiento", "procesador", "bater√≠a", "c√°mara"],
            "marca": ["marca", "apple", "samsung", "sony", "lenovo", "hp", "dell", "asus"]
        }
        
        response_lower = response.lower()
        
        for topic, keywords in topic_keywords.items():
            if any(keyword in response_lower for keyword in keywords):
                if topic not in self.conversation_context.topics_covered:
                    self.conversation_context.topics_covered.append(topic)
    
    def _analyze_user_intent(self, response: str):
        """
        Analiza la intenci√≥n y contexto profundo de la respuesta usando Gemini
        (M√©todo opcional para mejorar la comprensi√≥n del contexto)
        
        Args:
            response: √öltima respuesta del usuario
        """
        try:
            # Prompt para an√°lisis r√°pido de intenci√≥n
            intent_prompt = ChatPromptTemplate.from_messages([
                ("system", """Analiza BREVEMENTE la siguiente respuesta del usuario y extrae:
                1. Tema principal mencionado (una palabra: presupuesto/categor√≠a/uso/caracter√≠sticas/marca)
                2. Nivel de especificidad (bajo/medio/alto)
                3. Si menciona restricciones o preferencias fuertes
                
                Responde en formato: TEMA|ESPECIFICIDAD|RESTRICCIONES_SI_O_NO
                Ejemplo: "categoria|alto|si" o "presupuesto|medio|no"
                """),
                ("user", f"Respuesta: {response}")
            ])
            
            chain = intent_prompt | self.llm
            result = chain.invoke({})
            analysis = result.content.strip().lower()
            
            # Guardar an√°lisis en memoria para uso futuro
            self.update_memory(f"intent_analysis_{len(self.conversation_context.user_answers)}", analysis)
            
        except Exception as e:
            # Si falla el an√°lisis, continuar sin problema
            pass
    
    def _format_conversation_history(self) -> str:
        """
        Formatea el historial de la conversaci√≥n para el contexto
        
        Returns:
            Historial formateado
        """
        if not self.conversation_context.questions_asked:
            return "Conversaci√≥n reci√©n iniciada."
        
        history = []
        for i, (question, answer) in enumerate(zip(
            self.conversation_context.questions_asked,
            self.conversation_context.user_answers
        ), 1):
            history.append(f"Pregunta {i}: {question}")
            history.append(f"Respuesta {i}: {answer}")
            history.append("")
        
        return "\n".join(history)
    
    def has_more_questions(self) -> bool:
        """
        Verifica si hay m√°s preguntas por hacer
        
        Returns:
            True si puede hacer m√°s preguntas
        """
        return self.conversation_context.current_question_number < self.MAX_QUESTIONS
    
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Procesa toda la informaci√≥n recopilada y genera un resumen estructurado
        
        Args:
            input_data: Datos de entrada (opcional)
            
        Returns:
            Resumen estructurado de la informaci√≥n recopilada
        """
        conversation_history = self._format_conversation_history()
        
        # Prompt para analizar toda la conversaci√≥n
        summary_prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un analista experto en comprender necesidades de usuarios.
            Analiza la siguiente conversaci√≥n y extrae informaci√≥n estructurada sobre:
            
            1. **Categor√≠a de producto**: Tipo de producto que busca
            2. **Presupuesto**: Rango de precio mencionado o impl√≠cito
            3. **Caracter√≠sticas prioritarias**: Qu√© caracter√≠sticas son m√°s importantes
            4. **Uso previsto**: Para qu√© necesita el producto
            5. **Preferencias espec√≠ficas**: Marcas, especificaciones t√©cnicas, etc.
            6. **Restricciones**: Limitaciones mencionadas
            7. **Informaci√≥n adicional**: Cualquier otro dato relevante
            
            Formato tu respuesta de manera clara y estructurada.
            Si alguna informaci√≥n no fue proporcionada, ind√≠calo."""),
            ("user", "Conversaci√≥n:\n\n{conversation}\n\nAnaliza y estructura esta informaci√≥n:")
        ])
        
        try:
            chain = summary_prompt | self.llm
            result = chain.invoke({
                "conversation": conversation_history
            })
            
            summary = result.content
            
            # Guardar en memoria
            self.update_memory("conversation_history", conversation_history)
            self.update_memory("analysis", summary)
            self.update_memory("questions_asked", self.conversation_context.questions_asked)
            self.update_memory("user_answers", self.conversation_context.user_answers)
            
            return {
                "agent": self.name,
                "status": "completed",
                "questions_asked": len(self.conversation_context.questions_asked),
                "conversation_history": conversation_history,
                "structured_analysis": summary,
                "topics_covered": self.conversation_context.topics_covered
            }
            
        except Exception as e:
            return {
                "agent": self.name,
                "status": "error",
                "error": str(e),
                "conversation_history": conversation_history
            }
    
    def reset(self):
        """Reinicia el agente para una nueva sesi√≥n"""
        self.conversation_context = ConversationContext()
        self.clear_memory()
    
    def get_progress(self) -> str:
        """
        Obtiene el progreso actual de las preguntas
        
        Returns:
            String con el progreso (ej: "3/5")
        """
        return f"{self.conversation_context.current_question_number}/{self.MAX_QUESTIONS}"
    
    def get_summary(self) -> str:
        """
        Obtiene un resumen r√°pido de la informaci√≥n recopilada hasta ahora
        
        Returns:
            Resumen de la informaci√≥n
        """
        if not self.conversation_context.user_answers:
            return "No se ha recopilado informaci√≥n a√∫n."
        
        return f"""
üìä Informaci√≥n recopilada:
- Preguntas realizadas: {len(self.conversation_context.questions_asked)}
- Respuestas obtenidas: {len(self.conversation_context.user_answers)}
- Temas cubiertos: {', '.join(self.conversation_context.topics_covered) if self.conversation_context.topics_covered else 'Ninguno espec√≠fico'}
"""

